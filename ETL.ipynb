{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "import psycopg2\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# ==========================\n",
    "# Configuration and Setup\n",
    "# ==========================\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Process with OVERRIDING SYSTEM VALUE\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"SparkSession created successfully.\")\n",
    "\n",
    "# Database Configuration\n",
    "jdbc_url = \"jdbc:postgresql://172.27.253.185:5430/postgres_db\"\n",
    "db_properties = {\n",
    "    \"user\": \"postgres_user\",\n",
    "    \"password\": \"postgres_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# PostgreSQL Connection Parameters for psycopg2\n",
    "pg_conn_params = {\n",
    "    \"dbname\": \"postgres_db\",\n",
    "    \"user\": \"postgres_user\",\n",
    "    \"password\": \"postgres_password\",\n",
    "    \"host\": \"172.27.253.185\",\n",
    "    \"port\": 5430\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# Helper Functions\n",
    "# ==========================\n",
    "\n",
    "def upsert_with_overriding_system_value(source_df, target_table, conflict_column, exclude_columns=[]):\n",
    "    \"\"\"\n",
    "    Performs an upsert operation on the target_table using data from source_df.\n",
    "    Excludes specified columns from being updated to accommodate GENERATED ALWAYS constraints.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_df (DataFrame): Source Spark DataFrame to upsert.\n",
    "    - target_table (str): Target table in the database.\n",
    "    - conflict_column (str): Column to detect conflicts (usually primary key).\n",
    "    - exclude_columns (list): List of columns to exclude from the UPDATE operation.\n",
    "    \"\"\"\n",
    "    temp_table = f\"{target_table}_temp\"\n",
    "    try:\n",
    "        logger.info(f\"Writing data to temporary table {temp_table}.\")\n",
    "        source_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", temp_table) \\\n",
    "            .option(\"user\", db_properties[\"user\"]) \\\n",
    "            .option(\"password\", db_properties[\"password\"]) \\\n",
    "            .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "\n",
    "        # Generate column lists with proper quoting to handle case sensitivity and reserved keywords\n",
    "        source_columns = \", \".join([f'\"{c}\"' for c in source_df.columns])\n",
    "        update_columns = [\n",
    "            f'\"{c}\" = EXCLUDED.\"{c}\"' for c in source_df.columns\n",
    "            if c != conflict_column and c not in exclude_columns\n",
    "        ]\n",
    "\n",
    "        # Formulate SQL statement\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO {target_table}\n",
    "        ({source_columns})\n",
    "        OVERRIDING SYSTEM VALUE\n",
    "        SELECT {source_columns}\n",
    "        FROM {temp_table}\n",
    "        ON CONFLICT ({conflict_column})\n",
    "        DO UPDATE\n",
    "        SET {\", \".join(update_columns)};\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"Executing SQL: {insert_sql}\")  # Added SQL logging\n",
    "\n",
    "        # Connect to PostgreSQL and execute the upsert\n",
    "        with psycopg2.connect(**pg_conn_params) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(insert_sql)\n",
    "                conn.commit()\n",
    "                logger.info(f\"Upsert completed successfully for table {target_table}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during upsert operation for table {target_table}: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Optional: Drop the temporary table if no longer needed\n",
    "        try:\n",
    "            with psycopg2.connect(**pg_conn_params) as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute(f'DROP TABLE IF EXISTS {temp_table};')\n",
    "                    conn.commit()\n",
    "                    logger.info(f\"Temporary table {temp_table} dropped successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not drop temporary table {temp_table}: {e}\")\n",
    "\n",
    "def list_tables(schema):\n",
    "    \"\"\"\n",
    "    Retrieves the list of table names in the specified schema.\n",
    "    \n",
    "    Parameters:\n",
    "    - schema (str): The schema name.\n",
    "    \n",
    "    Returns:\n",
    "    - list: List of table names.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT table_name\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema = '{schema}';\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with psycopg2.connect(**pg_conn_params) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                tables = cursor.fetchall()\n",
    "                return [table[0] for table in tables]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving tables in schema {schema}: {e}\")\n",
    "        return []\n",
    "\n",
    "# ==========================\n",
    "# ETL Process\n",
    "# ==========================\n",
    "\n",
    "try:\n",
    "    # Load data from ROW layer\n",
    "    logger.info(\"Loading data from ROW layer.\")\n",
    "\n",
    "    # Define all row layer tables\n",
    "    row_layer_tables = [\n",
    "        \"source1.craft_market_wide\",\n",
    "        \"source2.craft_market_masters_products\",\n",
    "        \"source2.craft_market_orders_customers\",\n",
    "        \"source3.craft_market_craftsmans\",\n",
    "        \"source3.craft_market_customers\",\n",
    "        \"source3.craft_market_orders\"\n",
    "    ]\n",
    "\n",
    "    # Verify existence of tables in their respective schemas\n",
    "    for table in row_layer_tables:\n",
    "        schema, table_name = table.split('.')\n",
    "        existing_tables = list_tables(schema)\n",
    "        if table_name not in existing_tables:\n",
    "            logger.error(f\"Table {table} does not exist in schema {schema}.\")\n",
    "            raise Exception(f\"Table {table} does not exist in schema {schema}.\")\n",
    "        else:\n",
    "            logger.info(f\"Table {table} found in schema {schema}.\")\n",
    "\n",
    "    # Read all row layer tables into separate DataFrames\n",
    "    row_layer_data = {}\n",
    "    for table in row_layer_tables:\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {table}.\")\n",
    "            df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", jdbc_url) \\\n",
    "                .option(\"dbtable\", table) \\\n",
    "                .option(\"user\", db_properties[\"user\"]) \\\n",
    "                .option(\"password\", db_properties[\"password\"]) \\\n",
    "                .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "                .load()\n",
    "            row_layer_data[table] = df\n",
    "            logger.info(f\"Data loaded from {table} successfully.\")\n",
    "        except Exception as table_e:\n",
    "            logger.error(f\"Failed to load data from {table}: {table_e}\")\n",
    "            raise  # Re-raise to trigger the outer exception handler\n",
    "\n",
    "    # ==========================\n",
    "    # Process source1.craft_market_wide\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source1.craft_market_wide.\")\n",
    "    source1_df = row_layer_data[\"source1.craft_market_wide\"]\n",
    "\n",
    "    # Process d_craftsmans\n",
    "    d_craftsmans = source1_df.select(\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "        col(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_craftsmans,\n",
    "        target_table=\"dwh.d_craftsmans\",\n",
    "        conflict_column=\"craftsman_id\"\n",
    "    )\n",
    "\n",
    "    # Process d_customers\n",
    "    d_customers = source1_df.select(\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"customer_name\").alias(\"customer_name\"),\n",
    "        col(\"customer_address\").alias(\"customer_address\"),\n",
    "        col(\"customer_birthday\").alias(\"customer_birthday\"),\n",
    "        col(\"customer_email\").alias(\"customer_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_customers,\n",
    "        target_table=\"dwh.d_customers\",\n",
    "        conflict_column=\"customer_id\"\n",
    "    )\n",
    "\n",
    "    # Process d_products\n",
    "    d_products = source1_df.select(\n",
    "        col(\"product_id\").alias(\"product_id\"),\n",
    "        col(\"product_name\").alias(\"product_name\"),\n",
    "        col(\"product_description\").alias(\"product_description\"),\n",
    "        col(\"product_type\").alias(\"product_type\"),\n",
    "        col(\"product_price\").alias(\"product_price\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_products,\n",
    "        target_table=\"dwh.d_products\",\n",
    "        conflict_column=\"product_id\"\n",
    "    )\n",
    "\n",
    "    # Process f_orders\n",
    "    f_orders = source1_df.select(\n",
    "        col(\"order_id\").alias(\"order_id\"),\n",
    "        col(\"product_id\").alias(\"product_id\"),  # GENERATED ALWAYS column\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"order_created_date\").alias(\"order_created_date\"),\n",
    "        col(\"order_completion_date\").alias(\"order_completion_date\"),\n",
    "        col(\"order_status\").alias(\"order_status\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    )\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=f_orders,\n",
    "        target_table=\"dwh.f_orders\",\n",
    "        conflict_column=\"order_id\",\n",
    "        exclude_columns=[\"product_id\"]  # Exclude product_id from updates\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    # Process source2.craft_market_masters_products\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source2.craft_market_masters_products.\")\n",
    "    source2_df = row_layer_data[\"source2.craft_market_masters_products\"]\n",
    "\n",
    "    # Process d_craftsmans from source2\n",
    "    d_craftsmans_s2 = source2_df.select(\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "        col(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_craftsmans_s2,\n",
    "        target_table=\"dwh.d_craftsmans\",\n",
    "        conflict_column=\"craftsman_id\"\n",
    "    )\n",
    "\n",
    "    # Process d_products from source2\n",
    "    d_products_s2 = source2_df.select(\n",
    "        col(\"product_id\").alias(\"product_id\"),\n",
    "        col(\"product_name\").alias(\"product_name\"),\n",
    "        col(\"product_description\").alias(\"product_description\"),\n",
    "        col(\"product_type\").alias(\"product_type\"),\n",
    "        col(\"product_price\").alias(\"product_price\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_products_s2,\n",
    "        target_table=\"dwh.d_products\",\n",
    "        conflict_column=\"product_id\"\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    # Process source2.craft_market_orders_customers\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source2.craft_market_orders_customers.\")\n",
    "    source2_orders_customers_df = row_layer_data[\"source2.craft_market_orders_customers\"]\n",
    "\n",
    "    # Предполагаем, что эта таблица содержит информацию о заказах и клиентах\n",
    "    # Если в ней содержатся данные, аналогичные source3.craft_market_orders и source3.craft_market_customers,\n",
    "    # необходимо обработать их соответственно\n",
    "\n",
    "    # Обработка d_customers из source2.craft_market_orders_customers\n",
    "    d_customers_s2_orders = source2_orders_customers_df.select(\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"customer_name\").alias(\"customer_name\"),\n",
    "        col(\"customer_address\").alias(\"customer_address\"),\n",
    "        col(\"customer_birthday\").alias(\"customer_birthday\"),\n",
    "        col(\"customer_email\").alias(\"customer_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_customers_s2_orders,\n",
    "        target_table=\"dwh.d_customers\",\n",
    "        conflict_column=\"customer_id\"\n",
    "    )\n",
    "\n",
    "    # Обработка f_orders из source2.craft_market_orders_customers\n",
    "    f_orders_s2_orders_customers = source2_orders_customers_df.select(\n",
    "        col(\"order_id\").alias(\"order_id\"),\n",
    "        col(\"product_id\").alias(\"product_id\"),  # GENERATED ALWAYS column\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"order_created_date\").alias(\"order_created_date\"),\n",
    "        col(\"order_completion_date\").alias(\"order_completion_date\"),\n",
    "        col(\"order_status\").alias(\"order_status\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    )\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=f_orders_s2_orders_customers,\n",
    "        target_table=\"dwh.f_orders\",\n",
    "        conflict_column=\"order_id\",\n",
    "        exclude_columns=[\"product_id\"]  # Exclude product_id from updates\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    # Process source3.craft_market_craftsmans\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source3.craft_market_craftsmans.\")\n",
    "    source3_craftsmans_df = row_layer_data[\"source3.craft_market_craftsmans\"]\n",
    "\n",
    "    d_craftsmans_s3 = source3_craftsmans_df.select(\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"craftsman_name\").alias(\"craftsman_name\"),\n",
    "        col(\"craftsman_address\").alias(\"craftsman_address\"),\n",
    "        col(\"craftsman_birthday\").alias(\"craftsman_birthday\"),\n",
    "        col(\"craftsman_email\").alias(\"craftsman_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_craftsmans_s3,\n",
    "        target_table=\"dwh.d_craftsmans\",\n",
    "        conflict_column=\"craftsman_id\"\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    # Process source3.craft_market_customers\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source3.craft_market_customers.\")\n",
    "    source3_customers_df = row_layer_data[\"source3.craft_market_customers\"]\n",
    "\n",
    "    d_customers_s3 = source3_customers_df.select(\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"customer_name\").alias(\"customer_name\"),\n",
    "        col(\"customer_address\").alias(\"customer_address\"),\n",
    "        col(\"customer_birthday\").alias(\"customer_birthday\"),\n",
    "        col(\"customer_email\").alias(\"customer_email\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    ).distinct()\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=d_customers_s3,\n",
    "        target_table=\"dwh.d_customers\",\n",
    "        conflict_column=\"customer_id\"\n",
    "    )\n",
    "\n",
    "    # ==========================\n",
    "    # Process source3.craft_market_orders\n",
    "    # ==========================\n",
    "    logger.info(\"Processing table source3.craft_market_orders.\")\n",
    "    source3_orders_df = row_layer_data[\"source3.craft_market_orders\"]\n",
    "\n",
    "    f_orders_s3 = source3_orders_df.select(\n",
    "        col(\"order_id\").alias(\"order_id\"),\n",
    "        col(\"product_id\").alias(\"product_id\"),  # GENERATED ALWAYS column\n",
    "        col(\"craftsman_id\").alias(\"craftsman_id\"),\n",
    "        col(\"customer_id\").alias(\"customer_id\"),\n",
    "        col(\"order_created_date\").alias(\"order_created_date\"),\n",
    "        col(\"order_completion_date\").alias(\"order_completion_date\"),\n",
    "        col(\"order_status\").alias(\"order_status\"),\n",
    "        current_timestamp().alias(\"load_dttm\")\n",
    "    )\n",
    "\n",
    "    upsert_with_overriding_system_value(\n",
    "        source_df=f_orders_s3,\n",
    "        target_table=\"dwh.f_orders\",\n",
    "        conflict_column=\"order_id\",\n",
    "        exclude_columns=[\"product_id\"]  # Exclude product_id from updates\n",
    "    )\n",
    "\n",
    "    logger.info(\"ETL Process with OVERRIDING SYSTEM VALUE Completed Successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"ETL process failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n",
    "    logger.info(\"SparkSession stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
