{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обновление хранилища из источников данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, col, lit\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Параметры подключения для psycopg2\n",
    "pg_conn_params = {\n",
    "    \"dbname\": \"postgres_db\",\n",
    "    \"user\": \"postgres_user\",\n",
    "    \"password\": \"postgres_password\",\n",
    "    \"host\": \"172.27.253.185\",\n",
    "    \"port\": 5430  # Целое число для psycopg2\n",
    "}\n",
    "\n",
    "# Параметры подключения для Spark JDBC\n",
    "spark_jdbc_properties = {\n",
    "    \"user\": pg_conn_params[\"user\"],\n",
    "    \"password\": pg_conn_params[\"password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{pg_conn_params['host']}:{pg_conn_params['port']}/{pg_conn_params['dbname']}\"\n",
    "\n",
    "# Инициализация SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Process with Incremental Loading\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"SparkSession создан успешно.\")\n",
    "\n",
    "# Функция для чтения таблицы из PostgreSQL\n",
    "def read_table(schema, table, columns=\"*\"):\n",
    "    table_full = f\"{schema}.{table}\"\n",
    "    try:\n",
    "        df = spark.read.jdbc(url=jdbc_url, table=table_full, properties=spark_jdbc_properties)\n",
    "        if columns != \"*\":\n",
    "            df = df.select(*[c.strip() for c in columns.split(\",\")])\n",
    "        logger.info(f\"Таблица {table_full} успешно прочитана.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при чтении таблицы {table_full}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Функция для upsert\n",
    "def upsert_partition(iterator, pg_conn_params, schema, table, conflict_fields):\n",
    "    \"\"\"\n",
    "    Выполняет upsert данных из партиции в PostgreSQL.\n",
    "    \n",
    "    :param iterator: Итератор по строкам партиции\n",
    "    :param pg_conn_params: Параметры подключения PostgreSQL\n",
    "    :param schema: Схема таблицы\n",
    "    :param table: Имя таблицы\n",
    "    :param conflict_fields: Список колонок для конфликта\n",
    "    \"\"\"\n",
    "    # Преобразуем партицию в список записей\n",
    "    records = list(iterator)\n",
    "    if not records:\n",
    "        return\n",
    "    \n",
    "    # Определите поля и значения для вставки\n",
    "    columns = records[0].keys()\n",
    "    values = [[record[col] for col in columns] for record in records]\n",
    "    \n",
    "    # Создайте строку полей для SQL-запроса\n",
    "    columns_sql = \", \".join([f'\"{col}\"' for col in columns])\n",
    "    \n",
    "    # Создайте строку плейсхолдеров для значений\n",
    "    placeholders = \", \".join([\"%s\"] * len(columns))\n",
    "    \n",
    "    # Создайте строку для обновления при конфликте\n",
    "    update_fields = [f'\"{col}\" = EXCLUDED.\"{col}\"' for col in columns if col not in conflict_fields]\n",
    "    update_sql = \", \".join(update_fields)\n",
    "    \n",
    "    # Сформируйте полный SQL-запрос для upsert\n",
    "    conflict_sql = \", \".join([f'\"{field}\"' for field in conflict_fields])\n",
    "    upsert_query = f\"\"\"\n",
    "        INSERT INTO {schema}.{table} ({columns_sql})\n",
    "        VALUES ({placeholders})\n",
    "        ON CONFLICT ({conflict_sql})\n",
    "        DO UPDATE SET\n",
    "        {update_sql};\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Установите соединение с PostgreSQL\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=pg_conn_params[\"dbname\"],\n",
    "            user=pg_conn_params[\"user\"],\n",
    "            password=pg_conn_params[\"password\"],\n",
    "            host=pg_conn_params[\"host\"],\n",
    "            port=pg_conn_params[\"port\"]\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Выполните пакетную вставку с upsert\n",
    "        extras.execute_batch(cursor, upsert_query, values, page_size=100)\n",
    "        \n",
    "        # Зафиксируйте изменения\n",
    "        conn.commit()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при выполнении upsert: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# Функция для инкрементальной загрузки измерений\n",
    "def incremental_load(source_df, dwh_df, key, columns, dwh_schema, dwh_table, source_system_code):\n",
    "    \"\"\"\n",
    "    Выполняет инкрементальную загрузку измерений с использованием upsert через foreachPartition.\n",
    "    \n",
    "    :param source_df: DataFrame с исходными данными\n",
    "    :param dwh_df: DataFrame с данными DWH\n",
    "    :param key: Ключевое поле для сравнения\n",
    "    :param columns: Список столбцов для сравнения\n",
    "    :param dwh_schema: Схема DWH\n",
    "    :param dwh_table: Таблица DWH\n",
    "    :param source_system_code: Код системы источника (например, \"S1\", \"S2\", \"S3\")\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logger.info(f\"Начало инкрементальной загрузки для таблицы {dwh_schema}.{dwh_table} из источника {source_system_code}\")\n",
    "    \n",
    "    # Добавляем source_system_code к source_df\n",
    "    source_df = source_df.withColumn(\"source_system_code\", lit(source_system_code))\n",
    "    \n",
    "    # Выполнение LEFT JOIN для обнаружения новых или изменённых записей\n",
    "    joined_df = source_df.alias(\"src\").join(\n",
    "        dwh_df.alias(\"dwh\"),\n",
    "        on=(col(f\"src.{key}\") == col(f\"dwh.{key}\")) & \n",
    "           (col(\"src.source_system_code\") == col(\"dwh.source_system_code\")),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Формирование условия для выявления изменений\n",
    "    condition = lit(False)\n",
    "    for column in columns:\n",
    "        condition = condition | (col(f\"src.{column}\") != col(f\"dwh.{column}\"))\n",
    "    \n",
    "    # Фильтрация новых или изменённых записей\n",
    "    delta_df = joined_df.filter(condition | col(f\"dwh.{key}\").isNull()) \\\n",
    "                        .select([col(f\"src.{c}\") for c in source_df.columns]) \\\n",
    "                        .withColumn(\"load_dttm\", current_timestamp())\n",
    "    \n",
    "    count = delta_df.count()\n",
    "    if count > 0:\n",
    "        logger.info(f\"Найдено {count} новых или изменённых записей для таблицы {dwh_schema}.{dwh_table} из источника {source_system_code}. Выполнение upsert в DWH...\")\n",
    "        \n",
    "        # Определите конфликтующие поля (составной ключ)\n",
    "        conflict_fields = [\"source_system_code\", key]\n",
    "        \n",
    "        # Преобразуем DataFrame в RDD из словарей\n",
    "        delta_rdd = delta_df.rdd.map(lambda row: row.asDict())\n",
    "        \n",
    "        # Выполните upsert для каждой партиции\n",
    "        delta_rdd.foreachPartition(\n",
    "            lambda partition: upsert_partition(\n",
    "                partition,\n",
    "                pg_conn_params,  # Передаём параметры подключения psycopg2\n",
    "                dwh_schema,\n",
    "                dwh_table,\n",
    "                conflict_fields  # Передаём конфликтующие поля\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Upsert для таблицы {dwh_schema}.{dwh_table} из источника {source_system_code} завершен успешно.\")\n",
    "    else:\n",
    "        logger.info(f\"Нет новых или изменённых записей для таблицы {dwh_schema}.{dwh_table} из источника {source_system_code}.\")\n",
    "\n",
    "# Функция для инкрементальной загрузки фактов\n",
    "def incremental_load_f_orders(source_df, dwh_df, dwh_schema, dwh_table, source_system_code):\n",
    "    \"\"\"\n",
    "    Выполняет инкрементальную загрузку фактов f_orders с использованием upsert через foreachPartition.\n",
    "\n",
    "    :param source_df: DataFrame с исходными данными для факта\n",
    "    :param dwh_df: DataFrame с данными DWH для факта\n",
    "    :param dwh_schema: Схема DWH\n",
    "    :param dwh_table: Таблица DWH\n",
    "    :param source_system_code: Код системы источника (например, \"S1\", \"S2\", \"S3\")\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logger.info(f\"Начало инкрементальной загрузки факта {dwh_schema}.{dwh_table} из источника {source_system_code}\")\n",
    "    \n",
    "    # Добавляем source_system_code к source_df\n",
    "    source_df = source_df.withColumn(\"source_system_code\", lit(source_system_code))\n",
    "    \n",
    "    # Выполнение LEFT JOIN для обнаружения новых или изменённых записей\n",
    "    joined_fact_df = source_df.alias(\"src\").join(\n",
    "        dwh_df.alias(\"dwh\"),\n",
    "        (col(\"src.source_system_code\") == col(\"dwh.source_system_code\")) &\n",
    "        (col(\"src.order_id\") == col(\"dwh.order_id\")),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Определение условий для новых или изменённых записей\n",
    "    fact_condition = (\n",
    "        col(\"dwh.order_id\").isNull() |\n",
    "        (col(\"src.product_id\") != col(\"dwh.product_id\")) |\n",
    "        (col(\"src.craftsman_id\") != col(\"dwh.craftsman_id\")) |\n",
    "        (col(\"src.customer_id\") != col(\"dwh.customer_id\")) |\n",
    "        (col(\"src.order_created_date\") != col(\"dwh.order_created_date\")) |\n",
    "        (col(\"src.order_completion_date\") != col(\"dwh.order_completion_date\")) |\n",
    "        (col(\"src.order_status\") != col(\"dwh.order_status\"))\n",
    "    )\n",
    "    \n",
    "    # Фильтрация новых или изменённых записей\n",
    "    fact_delta_df = joined_fact_df.filter(fact_condition) \\\n",
    "        .select(\n",
    "            col(\"src.source_system_code\"),\n",
    "            col(\"src.order_id\"),\n",
    "            col(\"src.product_id\"),\n",
    "            col(\"src.craftsman_id\"),\n",
    "            col(\"src.customer_id\"),\n",
    "            col(\"src.order_created_date\"),\n",
    "            col(\"src.order_completion_date\"),\n",
    "            col(\"src.order_status\"),\n",
    "            col(\"src.load_dttm\")\n",
    "        )\n",
    "    \n",
    "    fact_count = fact_delta_df.count()\n",
    "    if fact_count > 0:\n",
    "        logger.info(f\"Найдено {fact_count} новых или изменённых записей для факта {dwh_schema}.{dwh_table} из источника {source_system_code}. Выполнение upsert в DWH...\")\n",
    "        \n",
    "        # Определите конфликтующие поля для факта\n",
    "        conflict_fields = [\"source_system_code\", \"order_id\"]\n",
    "        \n",
    "        # Преобразуем DataFrame в RDD из словарей\n",
    "        fact_delta_rdd = fact_delta_df.rdd.map(lambda row: row.asDict())\n",
    "        \n",
    "        # Выполните upsert для каждой партиции\n",
    "        fact_delta_rdd.foreachPartition(\n",
    "            lambda partition: upsert_partition(\n",
    "                partition,\n",
    "                pg_conn_params,  # Передаём параметры подключения psycopg2\n",
    "                dwh_schema,\n",
    "                dwh_table,\n",
    "                conflict_fields  # Передаём конфликтующие поля\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Upsert для факта {dwh_schema}.{dwh_table} из источника {source_system_code} завершен успешно.\")\n",
    "    else:\n",
    "        logger.info(f\"Нет новых или изменённых записей для факта {dwh_schema}.{dwh_table} из источника {source_system_code}.\")\n",
    "\n",
    "# Загрузка таблиц источников\n",
    "logger.info(\"Загрузка таблиц источников...\")\n",
    "\n",
    "# Источник 1: S1 - source1.craft_market_wide (Предполагается, что этот источник содержит данные для d_craftsmans, d_customers, d_products, f_orders)\n",
    "source1_df = read_table(\"source1\", \"craft_market_wide\")\n",
    "\n",
    "# Источник 2: S2 - source2.craft_market_masters_products и source2.craft_market_orders_customers\n",
    "source2_masters_products_df = read_table(\"source2\", \"craft_market_masters_products\")\n",
    "source2_orders_customers_df = read_table(\"source2\", \"craft_market_orders_customers\")\n",
    "\n",
    "# Источник 3: S3 - source3.craft_market_craftsmans, source3.craft_market_customers, source3.craft_market_orders\n",
    "source3_craftsmans_df = read_table(\"source3\", \"craft_market_craftsmans\")\n",
    "source3_customers_df = read_table(\"source3\", \"craft_market_customers\")\n",
    "source3_orders_df = read_table(\"source3\", \"craft_market_orders\")\n",
    "\n",
    "logger.info(\"Таблицы источников загружены успешно.\")\n",
    "\n",
    "# Загрузка таблиц DWH\n",
    "logger.info(\"Загрузка таблиц DWH...\")\n",
    "\n",
    "dwh_d_craftsmans_df = read_table(\"dwh\", \"d_craftsmans\")\n",
    "dwh_d_customers_df = read_table(\"dwh\", \"d_customers\")\n",
    "dwh_d_products_df = read_table(\"dwh\", \"d_products\")\n",
    "dwh_f_orders_df = read_table(\"dwh\", \"f_orders\")\n",
    "\n",
    "logger.info(\"Таблицы DWH загружены успешно.\")\n",
    "\n",
    "# Инкрементальная загрузка измерений с upsert\n",
    "\n",
    "# 1. Измерение d_craftsmans из S1\n",
    "# Предполагается, что S1 содержит данные для d_craftsmans\n",
    "if \"craftsman_id\" in source1_df.columns:\n",
    "    incremental_load(\n",
    "        source_df=source1_df.select(\n",
    "            \"craftsman_id\",\n",
    "            \"craftsman_name\",\n",
    "            \"craftsman_address\",\n",
    "            \"craftsman_birthday\",\n",
    "            \"craftsman_email\"\n",
    "        ),\n",
    "        dwh_df=dwh_d_craftsmans_df,\n",
    "        key=\"craftsman_id\",\n",
    "        columns=[\"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"],\n",
    "        dwh_schema=\"dwh\",\n",
    "        dwh_table=\"d_craftsmans\",\n",
    "        source_system_code=\"S1\"\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Источник S1 не содержит данных для d_craftsmans.\")\n",
    "\n",
    "# 2. Измерение d_craftsmans из S2\n",
    "# Необходимо объединить source2_orders_customers_df с source2_masters_products_df по craftsman_id\n",
    "logger.info(\"Объединение данных S2 для d_craftsmans...\")\n",
    "if {\"craftsman_id\", \"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"}.issubset(source2_masters_products_df.columns):\n",
    "    # Объединяем таблицы по craftsman_id\n",
    "    source2_combined_craftsmans_df = source2_orders_customers_df.join(\n",
    "        source2_masters_products_df.select(\n",
    "            \"craftsman_id\",\n",
    "            \"craftsman_name\",\n",
    "            \"craftsman_address\",\n",
    "            \"craftsman_birthday\",\n",
    "            \"craftsman_email\"\n",
    "        ),\n",
    "        on=\"craftsman_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Удаляем дубликаты по craftsman_id\n",
    "    source2_combined_craftsmans_df = source2_combined_craftsmans_df.dropDuplicates([\"craftsman_id\"])\n",
    "    \n",
    "    # Проверяем наличие необходимых столбцов\n",
    "    required_columns = [\"craftsman_id\", \"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"]\n",
    "    if all(col in source2_combined_craftsmans_df.columns for col in required_columns):\n",
    "        incremental_load(\n",
    "            source_df=source2_combined_craftsmans_df.select(*required_columns),\n",
    "            dwh_df=dwh_d_craftsmans_df,\n",
    "            key=\"craftsman_id\",\n",
    "            columns=[\"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"],\n",
    "            dwh_schema=\"dwh\",\n",
    "            dwh_table=\"d_craftsmans\",\n",
    "            source_system_code=\"S2\"\n",
    "        )\n",
    "    else:\n",
    "        missing = [col for col in required_columns if col not in source2_combined_craftsmans_df.columns]\n",
    "        logger.warning(f\"Источник S2 не содержит необходимых столбцов для d_craftsmans: {missing}. Пропускаем загрузку из S2.\")\n",
    "else:\n",
    "    logger.warning(\"Источник S2 не содержит таблицы craft_market_masters_products с необходимыми столбцами для d_craftsmans. Пропускаем загрузку из S2.\")\n",
    "\n",
    "# 3. Измерение d_craftsmans из S3\n",
    "incremental_load(\n",
    "    source_df=source3_craftsmans_df.select(\n",
    "        \"craftsman_id\",\n",
    "        \"craftsman_name\",\n",
    "        \"craftsman_address\",\n",
    "        \"craftsman_birthday\",\n",
    "        \"craftsman_email\"\n",
    "    ),\n",
    "    dwh_df=dwh_d_craftsmans_df,\n",
    "    key=\"craftsman_id\",\n",
    "    columns=[\"craftsman_name\", \"craftsman_address\", \"craftsman_birthday\", \"craftsman_email\"],\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"d_craftsmans\",\n",
    "    source_system_code=\"S3\"\n",
    ")\n",
    "\n",
    "# 4. Измерение d_customers из S1\n",
    "# Предполагается, что S1 содержит данные для d_customers\n",
    "if \"customer_id\" in source1_df.columns:\n",
    "    incremental_load(\n",
    "        source_df=source1_df.select(\n",
    "            \"customer_id\",\n",
    "            \"customer_name\",\n",
    "            \"customer_address\",\n",
    "            \"customer_birthday\",\n",
    "            \"customer_email\"\n",
    "        ),\n",
    "        dwh_df=dwh_d_customers_df,\n",
    "        key=\"customer_id\",\n",
    "        columns=[\"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"],\n",
    "        dwh_schema=\"dwh\",\n",
    "        dwh_table=\"d_customers\",\n",
    "        source_system_code=\"S1\"\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Источник S1 не содержит данных для d_customers.\")\n",
    "\n",
    "# 5. Измерение d_customers из S2\n",
    "# Данные о заказчиках находятся в source2_orders_customers_df\n",
    "if {\"customer_id\", \"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"}.issubset(source2_orders_customers_df.columns):\n",
    "    incremental_load(\n",
    "        source_df=source2_orders_customers_df.select(\n",
    "            \"customer_id\",\n",
    "            \"customer_name\",\n",
    "            \"customer_address\",\n",
    "            \"customer_birthday\",\n",
    "            \"customer_email\"\n",
    "        ).dropDuplicates([\"customer_id\"]),\n",
    "        dwh_df=dwh_d_customers_df,\n",
    "        key=\"customer_id\",\n",
    "        columns=[\"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"],\n",
    "        dwh_schema=\"dwh\",\n",
    "        dwh_table=\"d_customers\",\n",
    "        source_system_code=\"S2\"\n",
    "    )\n",
    "else:\n",
    "    missing = [col for col in [\"customer_id\", \"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"] if col not in source2_orders_customers_df.columns]\n",
    "    logger.warning(f\"Источник S2 не содержит необходимых столбцов для d_customers: {missing}. Пропускаем загрузку из S2.\")\n",
    "\n",
    "# 6. Измерение d_customers из S3\n",
    "incremental_load(\n",
    "    source_df=source3_customers_df.select(\n",
    "        \"customer_id\",\n",
    "        \"customer_name\",\n",
    "        \"customer_address\",\n",
    "        \"customer_birthday\",\n",
    "        \"customer_email\"\n",
    "    ),\n",
    "    dwh_df=dwh_d_customers_df,\n",
    "    key=\"customer_id\",\n",
    "    columns=[\"customer_name\", \"customer_address\", \"customer_birthday\", \"customer_email\"],\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"d_customers\",\n",
    "    source_system_code=\"S3\"\n",
    ")\n",
    "\n",
    "# 7. Измерение d_products из S1\n",
    "# Предполагается, что S1 содержит данные для d_products\n",
    "if \"product_id\" in source1_df.columns:\n",
    "    incremental_load(\n",
    "        source_df=source1_df.select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_description\",\n",
    "            \"product_type\",\n",
    "            \"product_price\"\n",
    "        ),\n",
    "        dwh_df=dwh_d_products_df,\n",
    "        key=\"product_id\",\n",
    "        columns=[\"product_name\", \"product_description\", \"product_type\", \"product_price\"],\n",
    "        dwh_schema=\"dwh\",\n",
    "        dwh_table=\"d_products\",\n",
    "        source_system_code=\"S1\"\n",
    "    )\n",
    "else:\n",
    "    logger.warning(\"Источник S1 не содержит данных для d_products.\")\n",
    "\n",
    "# 8. Измерение d_products из S2\n",
    "# Данные о продуктах находятся в source2_masters_products_df\n",
    "if {\"product_id\", \"product_name\", \"product_description\", \"product_type\", \"product_price\"}.issubset(source2_masters_products_df.columns):\n",
    "    incremental_load(\n",
    "        source_df=source2_masters_products_df.select(\n",
    "            \"product_id\",\n",
    "            \"product_name\",\n",
    "            \"product_description\",\n",
    "            \"product_type\",\n",
    "            \"product_price\"\n",
    "        ),\n",
    "        dwh_df=dwh_d_products_df,\n",
    "        key=\"product_id\",\n",
    "        columns=[\"product_name\", \"product_description\", \"product_type\", \"product_price\"],\n",
    "        dwh_schema=\"dwh\",\n",
    "        dwh_table=\"d_products\",\n",
    "        source_system_code=\"S2\"\n",
    "    )\n",
    "else:\n",
    "    missing = [col for col in [\"product_id\", \"product_name\", \"product_description\", \"product_type\", \"product_price\"] if col not in source2_masters_products_df.columns]\n",
    "    logger.warning(f\"Источник S2 не содержит необходимых столбцов для d_products: {missing}. Пропускаем загрузку из S2.\")\n",
    "\n",
    "# 9. Измерение d_products из S3\n",
    "# В S3 нет отдельной таблицы для продуктов, поэтому извлекаем из craft_market_orders\n",
    "product_columns = [\"product_id\", \"product_name\", \"product_description\", \"product_type\", \"product_price\"]\n",
    "\n",
    "incremental_load(\n",
    "    source_df=source3_orders_df.select(*product_columns).dropDuplicates([\"product_id\"]),\n",
    "    dwh_df=dwh_d_products_df,\n",
    "    key=\"product_id\",\n",
    "    columns=[\"product_name\", \"product_description\", \"product_type\", \"product_price\"],\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"d_products\",\n",
    "    source_system_code=\"S3\"\n",
    ")\n",
    "\n",
    "# Инкрементальная загрузка фактов f_orders из S1\n",
    "fact_orders_source_df_S1 = source1_df.select(\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"craftsman_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\"\n",
    ").withColumn(\"load_dttm\", current_timestamp()) \\\n",
    " .withColumn(\"source_system_code\", lit(\"S1\"))  # Код системы источника S1\n",
    "\n",
    "incremental_load_f_orders(\n",
    "    source_df=fact_orders_source_df_S1,\n",
    "    dwh_df=dwh_f_orders_df,\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"f_orders\",\n",
    "    source_system_code=\"S1\"\n",
    ")\n",
    "\n",
    "# Инкрементальная загрузка фактов f_orders из S2\n",
    "fact_orders_source_df_S2 = source2_orders_customers_df.select(\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"craftsman_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\"\n",
    ").withColumn(\"load_dttm\", current_timestamp()) \\\n",
    " .withColumn(\"source_system_code\", lit(\"S2\"))  # Код системы источника S2\n",
    "\n",
    "incremental_load_f_orders(\n",
    "    source_df=fact_orders_source_df_S2,\n",
    "    dwh_df=dwh_f_orders_df,\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"f_orders\",\n",
    "    source_system_code=\"S2\"\n",
    ")\n",
    "\n",
    "# Инкрементальная загрузка фактов f_orders из S3\n",
    "fact_orders_source_df_S3 = source3_orders_df.select(\n",
    "    col(\"order_id\").alias(\"order_id\"),\n",
    "    col(\"product_id\"),\n",
    "    col(\"craftsman_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    \"order_created_date\",\n",
    "    \"order_completion_date\",\n",
    "    \"order_status\"\n",
    ").withColumn(\"load_dttm\", current_timestamp()) \\\n",
    " .withColumn(\"source_system_code\", lit(\"S3\"))  # Код системы источника S3\n",
    "\n",
    "incremental_load_f_orders(\n",
    "    source_df=fact_orders_source_df_S3,\n",
    "    dwh_df=dwh_f_orders_df,\n",
    "    dwh_schema=\"dwh\",\n",
    "    dwh_table=\"f_orders\",\n",
    "    source_system_code=\"S3\"\n",
    ")\n",
    "\n",
    "# Остановка SparkSession\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession остановлена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обновление витрины с агрегатами только по тем данным которые изменились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum, avg, count, when, expr, percentile_approx, coalesce, lit,\n",
    "    current_timestamp, date_trunc, floor, months_between, date_format\n",
    ")\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from psycopg2 import extras\n",
    "\n",
    "# ---------- Настройка логирования ----------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Создание SparkSession ----------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Incremental Aggregation with partial recalc\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"SparkSession создан успешно.\")\n",
    "\n",
    "# ---------- Параметры подключения PostgreSQL ----------\n",
    "pg_conn_params = {\n",
    "    \"dbname\": \"postgres_db\",\n",
    "    \"user\": \"postgres_user\",\n",
    "    \"password\": \"postgres_password\",\n",
    "    \"host\": \"172.27.253.185\",\n",
    "    \"port\": 5430\n",
    "}\n",
    "spark_jdbc_properties = {\n",
    "    \"user\": pg_conn_params[\"user\"],\n",
    "    \"password\": pg_conn_params[\"password\"],\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "jdbc_url = f\"jdbc:postgresql://{pg_conn_params['host']}:{pg_conn_params['port']}/{pg_conn_params['dbname']}\"\n",
    "\n",
    "def read_table(schema, table):\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=f\"{schema}.{table}\",\n",
    "        properties=spark_jdbc_properties\n",
    "    )\n",
    "    cnt = df.count()\n",
    "    logger.info(f\"Таблица {schema}.{table} прочитана ({cnt} строк).\")\n",
    "    return df\n",
    "\n",
    "# ---------- Функции для UPDATE / INSERT витрины ----------\n",
    "def update_partition(iterator, pg_conn_params, schema, table):\n",
    "    records = list(iterator)\n",
    "    if not records:\n",
    "        return\n",
    "\n",
    "    columns = list(records[0].keys())\n",
    "    if \"id\" not in columns:\n",
    "        return\n",
    "    columns.remove(\"id\")\n",
    "\n",
    "    set_clause = \", \".join([f'\"{col}\" = %s' for col in columns])\n",
    "    update_sql = f\"\"\"\n",
    "        UPDATE {schema}.{table}\n",
    "        SET {set_clause}\n",
    "        WHERE \"id\" = %s\n",
    "    \"\"\"\n",
    "\n",
    "    values = []\n",
    "    for rec in records:\n",
    "        row_values = [rec[c] for c in columns]  # поля для UPDATE\n",
    "        row_values.append(rec[\"id\"])            # id в WHERE\n",
    "        values.append(row_values)\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(**pg_conn_params)\n",
    "        cursor = conn.cursor()\n",
    "        extras.execute_batch(cursor, update_sql, values, page_size=100)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при UPDATE: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def insert_partition(iterator, pg_conn_params, schema, table):\n",
    "    records = list(iterator)\n",
    "    if not records:\n",
    "        return\n",
    "\n",
    "    columns = list(records[0].keys())\n",
    "    cols_sql = \", \".join([f'\"{c}\"' for c in columns])\n",
    "    placeholders = \", \".join([\"%s\"] * len(columns))\n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO {schema}.{table} ({cols_sql})\n",
    "        VALUES ({placeholders})\n",
    "    \"\"\"\n",
    "\n",
    "    values = []\n",
    "    for rec in records:\n",
    "        row_values = [rec[c] for c in columns]\n",
    "        values.append(row_values)\n",
    "\n",
    "    conn = None\n",
    "    cursor = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(**pg_conn_params)\n",
    "        cursor = conn.cursor()\n",
    "        extras.execute_batch(cursor, insert_sql, values, page_size=100)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при INSERT: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Получаем max_load_dttm из таблицы загрузок\n",
    "# 2) Читаем f_orders, d_craftsmans, d_customers, d_products\n",
    "# 3) Фильтруем changed_f_orders, changed_d_craftsmans, ...\n",
    "# 4) По изменившимся измерениям определяем \"затронутые\" факты,\n",
    "#    union с changed_f_orders -> impacted_f_orders_df\n",
    "# 5) Агрегируем impacted_f_orders_df (при join используем актуальные d_* таблицы целиком)\n",
    "# 6) UPDATE / INSERT витрины\n",
    "# 7) Добавляем запись о загрузке в load_dates\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# ---------- (1) max_load_dttm ----------\n",
    "load_dates_df = read_table(\"dwh\", \"load_dates_craftsman_report_datamart\")\n",
    "max_load_dttm = load_dates_df.selectExpr(\"MAX(load_dttm) as max_dt\").first()[\"max_dt\"]\n",
    "logger.info(f\"Последняя дата обновления витрины: {max_load_dttm}\")\n",
    "\n",
    "# ---------- (2) Чтение таблиц ----------\n",
    "f_orders_df     = read_table(\"dwh\", \"f_orders\")\n",
    "d_craftsmans_df = read_table(\"dwh\", \"d_craftsmans\")\n",
    "d_customers_df  = read_table(\"dwh\", \"d_customers\")\n",
    "d_products_df   = read_table(\"dwh\", \"d_products\")\n",
    "\n",
    "# ---------- (3) Фильтруем изменившиеся строки ----------\n",
    "def filter_by_load_dttm(df, max_dt):\n",
    "    if max_dt:\n",
    "        return df.filter(col(\"load_dttm\") > lit(max_dt))\n",
    "    else:\n",
    "        # Первая загрузка - все строки \"новые\"\n",
    "        return df\n",
    "\n",
    "changed_f_orders_df     = filter_by_load_dttm(f_orders_df,     max_load_dttm)\n",
    "changed_craftsmans_df   = filter_by_load_dttm(d_craftsmans_df, max_load_dttm)\n",
    "changed_customers_df    = filter_by_load_dttm(d_customers_df,  max_load_dttm)\n",
    "changed_products_df     = filter_by_load_dttm(d_products_df,   max_load_dttm)\n",
    "\n",
    "cnt_f  = changed_f_orders_df.count()\n",
    "cnt_dc = changed_craftsmans_df.count()\n",
    "cnt_du = changed_customers_df.count()\n",
    "cnt_dp = changed_products_df.count()\n",
    "\n",
    "logger.info(f\"Изменений: f_orders={cnt_f}, craftsmans={cnt_dc}, customers={cnt_du}, products={cnt_dp}\")\n",
    "\n",
    "# Если вообще нет изменений - выходим\n",
    "if (cnt_f + cnt_dc + cnt_du + cnt_dp) == 0:\n",
    "    logger.info(\"Нет новых/изменённых данных. Выходим.\")\n",
    "    spark.stop()\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ---------- (4) Определяем \"затронутые\" факты ----------\n",
    "# Начнём с фактов, которые сами изменились\n",
    "impacted_f_orders_df = changed_f_orders_df\n",
    "\n",
    "# a) мастера\n",
    "if cnt_dc > 0:\n",
    "    # берём все changed_craftsmans_df -> вытаскиваем их craftsmans_id\n",
    "    changed_craftsman_ids = changed_craftsmans_df.select(\"craftsman_id\").distinct()\n",
    "    # подтягиваем из f_orders все факты по этим id\n",
    "    # (не обязательно фильтровать по load_dttm, т.к. факт мог не меняться,\n",
    "    #  но измерение поменялось - нужно пересчитать)\n",
    "    impacted_from_craftsmans = (\n",
    "        f_orders_df\n",
    "        .join(changed_craftsman_ids, \"craftsman_id\", how=\"inner\")\n",
    "    )\n",
    "    impacted_f_orders_df = impacted_f_orders_df.unionByName(impacted_from_craftsmans)\n",
    "\n",
    "# b) customers\n",
    "if cnt_du > 0:\n",
    "    changed_customer_ids = changed_customers_df.select(\"customer_id\").distinct()\n",
    "    impacted_from_customers = (\n",
    "        f_orders_df\n",
    "        .join(changed_customer_ids, \"customer_id\", how=\"inner\")\n",
    "    )\n",
    "    impacted_f_orders_df = impacted_f_orders_df.unionByName(impacted_from_customers)\n",
    "\n",
    "# c) products\n",
    "if cnt_dp > 0:\n",
    "    changed_product_ids = changed_products_df.select(\"product_id\").distinct()\n",
    "    impacted_from_products = (\n",
    "        f_orders_df\n",
    "        .join(changed_product_ids, \"product_id\", how=\"inner\")\n",
    "    )\n",
    "    impacted_f_orders_df = impacted_f_orders_df.unionByName(impacted_from_products)\n",
    "\n",
    "# Убираем дубликаты заказов, чтобы не считать один и тот же order_id несколько раз\n",
    "# (зависит от вашей схемы, если order_id уникален в рамках source_system_code).\n",
    "dedup_columns = [\"order_id\", \"source_system_code\"] if \"source_system_code\" in impacted_f_orders_df.columns else [\"order_id\"]\n",
    "impacted_f_orders_df = impacted_f_orders_df.dropDuplicates(dedup_columns)\n",
    "\n",
    "cnt_impacted = impacted_f_orders_df.count()\n",
    "logger.info(f\"Итоговое кол-во затронутых фактов: {cnt_impacted}\")\n",
    "\n",
    "if cnt_impacted == 0:\n",
    "    logger.info(\"Нет фактов, затронутых изменениями (возможно данные удалили?). Выходим.\")\n",
    "    spark.stop()\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ---------- (5) Агрегируем только impacted_f_orders_df ----------\n",
    "# При этом для join с измерениями берём их *актуальную* (полную) версию,\n",
    "# чтобы получить последние данные по craftsmans, products, ...\n",
    "from pyspark.sql.functions import date_trunc\n",
    "\n",
    "impacted_f_orders_df = impacted_f_orders_df.withColumn(\n",
    "    \"report_period\", date_trunc(\"month\", col(\"order_created_date\"))\n",
    ")\n",
    "\n",
    "# добавляем возраст клиента\n",
    "impacted_f_orders_df = impacted_f_orders_df.join(\n",
    "    d_customers_df.select(\"customer_id\", \"customer_birthday\"),\n",
    "    on=\"customer_id\", how=\"left\"\n",
    ").withColumn(\n",
    "    \"customer_age\",\n",
    "    floor(months_between(col(\"order_created_date\"), col(\"customer_birthday\")) / 12)\n",
    ")\n",
    "\n",
    "joined_impacted_df = (\n",
    "    impacted_f_orders_df\n",
    "    .join(d_craftsmans_df, \"craftsman_id\", how=\"left\")\n",
    "    .join(d_products_df,   \"product_id\",   how=\"left\")\n",
    ")\n",
    "\n",
    "aggregated_data_df = (\n",
    "    joined_impacted_df\n",
    "    .groupBy(\"craftsman_id\", \"report_period\")\n",
    "    .agg(\n",
    "        expr(\"MAX(craftsman_name) AS craftsman_name\"),\n",
    "        expr(\"MAX(craftsman_address) AS craftsman_address\"),\n",
    "        expr(\"MAX(craftsman_birthday) AS craftsman_birthday\"),\n",
    "        expr(\"MAX(craftsman_email) AS craftsman_email\"),\n",
    "        coalesce(\n",
    "            sum(when(col(\"order_status\") == \"done\", col(\"product_price\") * 0.9)),\n",
    "            lit(0)\n",
    "        ).alias(\"craftsman_money\"),\n",
    "        coalesce(\n",
    "            sum(when(col(\"order_status\") == \"done\", col(\"product_price\") * 0.1)),\n",
    "            lit(0)\n",
    "        ).alias(\"platform_money\"),\n",
    "        count(\"order_id\").alias(\"count_order\"),\n",
    "        coalesce(\n",
    "            avg(when(col(\"order_status\") == \"done\", col(\"product_price\"))),\n",
    "            lit(0)\n",
    "        ).alias(\"avg_price_order\"),\n",
    "        coalesce(\n",
    "            avg(col(\"customer_age\")),\n",
    "            lit(0)\n",
    "        ).alias(\"avg_age_customer\"),\n",
    "        coalesce(\n",
    "            percentile_approx(\n",
    "                (col(\"order_completion_date\").cast(\"long\") - col(\"order_created_date\").cast(\"long\"))/86400,\n",
    "                0.5\n",
    "            ),\n",
    "            lit(0)\n",
    "        ).alias(\"median_time_order_completed\"),\n",
    "        expr(\"MAX(product_type) AS top_product_category\"),\n",
    "        count(when(col(\"order_status\") == \"created\", 1)).alias(\"count_order_created\"),\n",
    "        count(when(col(\"order_status\") == \"in progress\", 1)).alias(\"count_order_in_progress\"),\n",
    "        count(when(col(\"order_status\") == \"delivery\", 1)).alias(\"count_order_delivery\"),\n",
    "        count(when(col(\"order_status\") == \"done\", 1)).alias(\"count_order_done\"),\n",
    "        count(when(col(\"order_status\") != \"done\", 1)).alias(\"count_order_not_done\")\n",
    "    )\n",
    ")\n",
    "\n",
    "aggregated_data_df = aggregated_data_df.withColumn(\n",
    "    \"report_period\",\n",
    "    date_format(col(\"report_period\"), \"yyyy-MM\")\n",
    ")\n",
    "\n",
    "cnt_agg = aggregated_data_df.count()\n",
    "logger.info(f\"Подготовлено {cnt_agg} агрегированных записей для обновления витрины.\")\n",
    "\n",
    "# ---------- (6) UPDATE / INSERT в dwh.craftsman_report_datamart ----------\n",
    "datamart_df = read_table(\"dwh\", \"craftsman_report_datamart\")\n",
    "\n",
    "join_cond = [\n",
    "    aggregated_data_df[\"craftsman_id\"] == datamart_df[\"craftsman_id\"],\n",
    "    aggregated_data_df[\"report_period\"] == datamart_df[\"report_period\"]\n",
    "]\n",
    "joined_dm = aggregated_data_df.alias(\"new\").join(\n",
    "    datamart_df.alias(\"old\"), join_cond, how=\"left\"\n",
    ")\n",
    "\n",
    "to_update_df = (\n",
    "    joined_dm\n",
    "    .filter(col(\"old.id\").isNotNull())\n",
    "    .select(\n",
    "        col(\"old.id\").alias(\"id\"),\n",
    "        col(\"new.craftsman_id\"),\n",
    "        col(\"new.craftsman_name\"),\n",
    "        col(\"new.craftsman_address\"),\n",
    "        col(\"new.craftsman_birthday\"),\n",
    "        col(\"new.craftsman_email\"),\n",
    "        col(\"new.craftsman_money\"),\n",
    "        col(\"new.platform_money\"),\n",
    "        col(\"new.count_order\"),\n",
    "        col(\"new.avg_price_order\"),\n",
    "        col(\"new.avg_age_customer\"),\n",
    "        col(\"new.median_time_order_completed\"),\n",
    "        col(\"new.top_product_category\"),\n",
    "        col(\"new.count_order_created\"),\n",
    "        col(\"new.count_order_in_progress\"),\n",
    "        col(\"new.count_order_delivery\"),\n",
    "        col(\"new.count_order_done\"),\n",
    "        col(\"new.count_order_not_done\"),\n",
    "        col(\"new.report_period\")\n",
    "    )\n",
    ")\n",
    "\n",
    "to_insert_df = (\n",
    "    joined_dm\n",
    "    .filter(col(\"old.id\").isNull())\n",
    "    .select(\n",
    "        col(\"new.craftsman_id\"),\n",
    "        col(\"new.craftsman_name\"),\n",
    "        col(\"new.craftsman_address\"),\n",
    "        col(\"new.craftsman_birthday\"),\n",
    "        col(\"new.craftsman_email\"),\n",
    "        col(\"new.craftsman_money\"),\n",
    "        col(\"new.platform_money\"),\n",
    "        col(\"new.count_order\"),\n",
    "        col(\"new.avg_price_order\"),\n",
    "        col(\"new.avg_age_customer\"),\n",
    "        col(\"new.median_time_order_completed\"),\n",
    "        col(\"new.top_product_category\"),\n",
    "        col(\"new.count_order_created\"),\n",
    "        col(\"new.count_order_in_progress\"),\n",
    "        col(\"new.count_order_delivery\"),\n",
    "        col(\"new.count_order_done\"),\n",
    "        col(\"new.count_order_not_done\"),\n",
    "        col(\"new.report_period\")\n",
    "    )\n",
    ")\n",
    "\n",
    "cnt_update = to_update_df.count()\n",
    "cnt_insert = to_insert_df.count()\n",
    "logger.info(f\"К обновлению: {cnt_update}, к вставке: {cnt_insert}.\")\n",
    "\n",
    "# --- UPDATE\n",
    "to_update_rdd = to_update_df.rdd.map(lambda row: row.asDict())\n",
    "to_update_rdd.foreachPartition(\n",
    "    lambda part: update_partition(\n",
    "        part, pg_conn_params, schema=\"dwh\", table=\"craftsman_report_datamart\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- INSERT\n",
    "to_insert_rdd = to_insert_df.rdd.map(lambda row: row.asDict())\n",
    "to_insert_rdd.foreachPartition(\n",
    "    lambda part: insert_partition(\n",
    "        part, pg_conn_params, schema=\"dwh\", table=\"craftsman_report_datamart\"\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(\"Частичный пересчёт витрины (UPDATE + INSERT) завершён.\")\n",
    "\n",
    "# ---------- (7) Добавляем запись о времени загрузки ----------\n",
    "current_time = [(datetime.now(),)]\n",
    "load_dates_new_df = spark.createDataFrame(current_time, [\"load_dttm\"])\n",
    "load_dates_new_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dwh.load_dates_craftsman_report_datamart\",\n",
    "    mode=\"append\",\n",
    "    properties=spark_jdbc_properties\n",
    ")\n",
    "logger.info(\"Таблица дат загрузок обновлена (append).\")\n",
    "\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession остановлен.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
